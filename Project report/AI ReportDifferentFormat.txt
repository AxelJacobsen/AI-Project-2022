AI Project Report

Authors:
Axel E. W. Jacobsen
Daniel Hao Hyunh
Matthias Greeven
 

HELLO GRUPPA:
Har brutt ned oppgaven litt for å gjøre den mer overkommelig, ingenting her er satt i stein.
Om du begynner å skrive så finner du ut at noe passer bedre et annet sted, bare flytt det og kompanser. Undertittlene er ikke egt ment som egne avsnitt, men for de større kategoriene kan det være smart å splitte i to. Tenker det er smart å ha med en del grafer og bilder i teksten, men hold det til etter «Your implementation»
Bare sleng in når dere har gjort noe som kan være verdt å nevne på contributions siden.

Introduction             300-400 Words
    Task            (Project scope)
    Project motivation
This is the project report for the AI project spring 2022. For this project our goal was to create an AI model trained on a self-made dataset made up of 9 different hand gestures. Since the task is essentially a bullet point list, there are not too many assumptions to be made. However, the task asks for “More than 5” gestures, so we decided to go all out, and do 9. This shouldn’t cause any issues, due to there being 10000 photos in the final dataset. We came to this number of photos due to the task asking for “at least 2000 special hand gestures”, as well as “Create the same number of samples from other 4 users”. Since there were a lot of pictures to take, we contacted other groups to ensure we could cooperate through sharing photos, and thereby easing the workload. Finally, we decided to choose VGG19 as our primary attention-based CNN model and compare it to a ResNet network trained on the same data. Our goal is to perform an experiment by training two AI models with a self-made dataset and see which one performs better.
Background            100-300 Words
    Explain any technical feature of the project
Background information needed to understand the technical details in the rest of the report (?)
To understand how we executed this project, some terminology must be explained. An AI dataset is defined by the Oxford Dictionary as “a collection of data that is treated as a single unit by a computer”. The dataset we have made is essentially just a large collection of pictures with labels attached to each. The data in the dataset has been “normalized” to ensure more uniform data. Normalizing 
When the AI model reads this data it “looks” at the pictures and “memorizes” important features like the outline and edges.
Your implementation.                                 Alt under her faller under det
Pictures            300-450 Words
How we decided gestures
    Cooperation with other groups
    Problems we faced and potential issues
Before we started taking pictures, we had to decide on what gestures we wanted to use. Due to the magnitude of photos needed, we cooperated with another group from the start to agree upon which gestures we wanted to use. This way we had at least 3000 pictures with identical labels. The gestures we settled on were; “Closed_fist”, “Finger_guns”, “Open_palm”, “Peace_sign”, ”Pinky”, “Pointing”, “Rocknroll”, “Spiderman”, “Spock” and “Thumbs_up”. Most names are self-explanatory, but for “Spiderman” and “Spock”, they are each a reference to iconic hand gestures from pop culture. “Spiderman” is essentially just a flipped version of “Rocknroll” with the thumb sticking out. While “Spock” is the Vulcan salute from Star trek. The gesture is an alteration of “Open_palm”, where the fingers are split into three groups, index and middle finger, pinky, and ring finger, lastly, the thumb. 
Since there were a lot of pictures to take, everyone took the pictures individually. This led to a lot of noise in the form of varying camera quality, different lighting, and potential background objects. We concluded that this shouldn’t cause any significant issues in the training, since we are normalizing the data to grayscale, and a standardized size. However, another issue arose after taking the images. Transporting the images before they were normalized proved to be a challenge due to their size. We solved this by using google-photos. Google-photos is efficient at backing up photos from your phone, as well as supporting “bursts”. Using “bursts” allowed us to take hundreds of seconds in a matter of seconds. This significantly reduced the overall time it took us to process the images and construct the dataset.
Normalizing            200-400 Words
    How is the data normalized?
    Why did we do it like this
    Where did we find code for it?
 As mentioned, we realized that it was necessary to normalize our data. Before we started, we researched what common ways image data is usually normalized. We wanted the normalization to be automated with a script, since there was too much data to do manually. After taking this into consideration, we ended up finding a script which would resize and grayscale our data. We had to alter it quite significantly to have it work in the first place, but when we were done, it normalized and compiled it into a dataset contained within a class. However, even though the script worked, it was slow and inefficient compared to built-in solutions. After performing some research, we found that keras had a function called “image_dataset_from_directory”. This permitted adding shuffle as well as the aforementioned modifications. Using this method turned out to be faster and less memory dependent than the original script. With this, our data is efficiently normalized and loaded into the model at the start of each build.

AI models            250-600 Words
    Which we chose and why
    Where did we get the code?
When deciding what models to use we wanted to ensure that they were attention based, and efficient. Attention mimics cognitive recognition, as it focuses on certain parts of the image relevant to its previous results and diminishes other parts of the image deemed unnecessary. The idea behind it is to improve the recognition of the model, as it only focuses on the parts deemed necessary.
VGG19 stands for Visual Geometry Group, and the number on the back reveals how many layers are present. We chose to use this model because the layering architecture the model uses is good for identifying the smaller details on the images we provided. That proved very useful when the images only have a few distinctions in between them, and a few alterations within the same category. As it has 19 layers it’s a very extensive network, more so than its counterpart, the VGG16 model, which has fewer layers, which reduces training time, but could compromise accuracy due to fewer convolutional layers. In each of those layers there are specific filters applied that deconstruct the images into more and more classifiable data.
ResNet is a Convolutional Neural Network, and we chose to use the model with 152 layers (the largest one, as the images are complex). The model stacks the layers on top of each other in order to form an Artificial Neural Network. The reason we chose it is due to its renowned low error/loss rate. ResNet has a VGG inspired build, where the layers have filters that classify the data as it goes down, but it’s different due to the fewer filters and lower complexity. 
So we chose these models to compare the two main attributes, simplicity (ResNet) vs complexity (VGG19). In terms of Floating Point Operations Per Second (FLOPS), which decides how long it takes to train the model (not depending on the GPU requirements), we see that ResNet152 (11.3bn FLOPS) is a lot faster than VGG19 (19.6bn FLOPS). 
Experiments (?)        250-550 Words
    Datasets    (Mulig vi alrede har skrevet dette tidliger, usikker)
    Results        (Hva fant vi, typ accuracy og lignende)
    Analysis     (Blir vel litt dypere hvorfor vi fikk svarene vi fikk)

Reflection & discussion    200-400 Words
comparisons     (Åssen performa datasettet på en model ovenfor en annen)
discussion    (Hva kunne vi gjort bedre, hva funka hva funka ikke)
Conclusion            200-500 Words


Sources:
https://www.oxfordlearnersdictionaries.com/definition/english/data-set?q=data+set
https://viso.ai/deep-learning/vgg-very-deep-convolutional-networks/
https://viso.ai/deep-learning/resnet-residual-neural-network/
 
Group contributions:
Everyone has contributed pictures for the dataset.
Individual contributions:
Axel:
    Report structuring
    Finding VGG19 model
Daniel:
    Finding and implementing data normalization
Matthias:
    Report implementation
    Implementing ResNet model
